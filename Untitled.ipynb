{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6222664015904572\n",
      "Text : อาจารย์เป็นอาจารย์ที่มหาวิทยาลัยขอนแก่น\n",
      "[('อาจารย์', 'B-NP-PERSON'), ('เป็น', 'O'), ('อาจารย์', 'O'), ('ที่', 'O'), ('มหาวิทยาลัยขอนแก่น', 'B-NP-UNIVERSITY')]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "NER ภาษาไทย\n",
    "พัฒนาโดย นาย วรรณพงษ์ ภัททิยไพบูลย์\n",
    "\"\"\"\n",
    "import argparse\n",
    "import codecs\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.tag import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "import re\n",
    "pattern = r'\\[(.*?)\\](.*?)\\[\\/(.*?)\\]'\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "def text2conll2002(text):\n",
    "    \"\"\"\n",
    "    ใช้แปลงข้อความให้กลายเป็น conll2002\n",
    "    \"\"\"\n",
    "    text=text.replace(' ','<space>')\n",
    "    text=text.replace(\"''\",'\"')\n",
    "    text=text.replace(\"’\",'\"').replace(\"‘\",'\"')#.replace('\"',\"\")\n",
    "    tag=tokenizer.tokenize(text)\n",
    "    #text2 = word_tokenize(re.sub('\\[.*?]','',text))\n",
    "    #print(text2)\n",
    "    #pos_tag2=pos_tag(text2,engine='artagger')\n",
    "    #print(len([i for i in pos_tag2]))\n",
    "    j=0\n",
    "    conll2002=\"\"\n",
    "    for tagopen,text,tagclose in tag:\n",
    "        word_cut=word_tokenize(text)\n",
    "        #print(word_cut)\n",
    "        #print(j)\n",
    "        i=0\n",
    "        while i<len(word_cut):\n",
    "            if word_cut[i]==\"''\" or word_cut[i]=='\"':pass\n",
    "            elif i==0 and tagopen!='word':\n",
    "                conll2002+=word_cut[i]\n",
    "                #conll2002+='\\t'+pos_tag2[j][1]\n",
    "                conll2002+='\\t'+'B-'+'NP-' +tagopen #tagopen\n",
    "            elif tagopen!='word':\n",
    "                conll2002+=word_cut[i]\n",
    "                #conll2002+='\\t'+pos_tag2[j][1]\n",
    "                conll2002+='\\t'+'I-'+'NP'#tagopen\n",
    "            else:\n",
    "                conll2002+=word_cut[i]\n",
    "                #conll2002+='\\t'+pos_tag2[j][1]\n",
    "                conll2002+='\\t'+'O'\n",
    "            conll2002+='\\n'\n",
    "            #j+=1\n",
    "            i+=1\n",
    "    return postag(conll2002)\n",
    "def postag(text):\n",
    "    listtxt=[i for i in text.split('\\n') if i!='']\n",
    "    list_word=[]\n",
    "    for data in listtxt:\n",
    "        list_word.append(data.split('\\t')[0])\n",
    "    #print(text)\n",
    "    list_word=pos_tag(list_word,engine='artagger')\n",
    "    text=\"\"\n",
    "    i=0\n",
    "    for data in listtxt:\n",
    "        text+=data.split('\\t')[0]+'\\t'+list_word[i][1]+'\\t'+data.split('\\t')[1]+'\\n'\n",
    "        i+=1\n",
    "    return text\n",
    "def get_data(fileopen):\n",
    "\t\"\"\"\n",
    "    สำหรับใช้อ่านทั้งหมดทั้งในไฟล์ทีละรรทัดออกมาเป็น list\n",
    "    \"\"\"\n",
    "\twith codecs.open(fileopen, 'r',encoding='utf8') as f:\n",
    "\t\tlines = f.read().splitlines()\n",
    "\treturn lines\n",
    "def alldata(lists):\n",
    "    text=\"\"\n",
    "    for data in lists:\n",
    "        text+=text2conll2002(data)\n",
    "        text+='\\n'\n",
    "    return text\n",
    "def alldata_list(lists):\n",
    "    data_all=[]\n",
    "    for data in lists:\n",
    "        data_num=[]\n",
    "        txt=text2conll2002(data).split('\\n')\n",
    "        for d in txt:\n",
    "            tt=d.split('\\t')\n",
    "            if d!=\"\": data_num.append((tt[0],tt[1],tt[2]))\n",
    "        #print(data_num)\n",
    "        data_all.append(data_num)\n",
    "    #print(data_all)\n",
    "    return data_all\n",
    "def write_conll2002(file_name,data):\n",
    "    \"\"\"\n",
    "    ใช้สำหรับเขียนไฟล์\n",
    "    \"\"\"\n",
    "    with codecs.open(file_name, \"w\", \"utf-8-sig\") as temp:\n",
    "        temp.write(data)\n",
    "    return True\n",
    "class TrainChunker(nltk.ChunkParserI):\n",
    "    \"\"\"\n",
    "    ใช้ในการ Train และรัน\n",
    "    \"\"\"\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in sent] for sent in train_sents]\n",
    "        #print(train_data)\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "        self.tagger = nltk.tag.BigramTagger(train_data, backoff=self.tagger)\n",
    "        self.tagger = nltk.tag.TrigramTagger(train_data, backoff=self.tagger)\n",
    "        print(self.tagger.evaluate(train_data))\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word.replace('<space>',' '), pos, chunktag) for ((word,pos),chunktag) in zip(sentence, chunktags)]\n",
    "        #print(conlltags)\n",
    "        return conlltags\n",
    "def run(lists):\n",
    "    \"\"\"\n",
    "    ใช้ในการทดสอบ NER\n",
    "    \"\"\"\n",
    "    data=alldata_list(lists)\n",
    "    tag=TrainChunker(data)\n",
    "    while True:\n",
    "        texts=input(\"Text : \")\n",
    "        if texts == 'exit':\n",
    "            break\n",
    "        toword=word_tokenize(texts.replace(' ','<space>'))\n",
    "        pos=pos_tag(toword,engine='artagger')\n",
    "        ner=tag.parse(pos)\n",
    "        print([(word, chunktag) for (word,pos,chunktag) in ner])\n",
    "if __name__ == '__main__':\n",
    "    listdata=get_data(\"general.text\")\n",
    "    run(listdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time = re.search(r'[\\d]{2}[:][\\d]{2}',content)\n",
    "if time != None :\n",
    "    time=time.group(0)\n",
    "else :\n",
    "    time= \"No Time\"\n",
    "date = re.search(r'[\\d]{2}[/][\\d]{2}[/][\\d]{4}',content)\n",
    "if date != None :\n",
    "    date=date.group(0)\n",
    "else :\n",
    "    date= \"No date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "person=re.search(r'[\\S\\.-][.,]?[\\S\\.-]+',content)\n",
    "if person != None:\n",
    "    person=person.group(0)\n",
    "else:\n",
    "    person=\"No Person\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12/02/2018'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = 'วันที่ 12/02/2018 ได้มีการบลาๆ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    content = content.split(\" \")\n",
    "    email = []\n",
    "    for i in range(len(content)):\n",
    "        match = re.search(r'[\\w\\.-]+@[\\w\\.-]+',content[i])\n",
    "        if match != None:\n",
    "           matches=match.group()\n",
    "        else:\n",
    "            matches = None\n",
    "            \n",
    "        email.append(matches)\n",
    "        Email=[x for x in email if x is not None]\n",
    "        \n",
    "    return jsonify({'Email':Email})    \n",
    "\n",
    "\n",
    "@app.route('/api/ner', methods=['POST'])\n",
    "def parse_request3():\n",
    "    \n",
    "    task1= request.get_data()\n",
    "    task=task1.decode(\"utf-8\")    \n",
    "    task2=task.split(\"name=\")\n",
    "    task3=task2[1].split(\"\"\"\\\"\\r\\n\\r\\n\\r\\n---\"\"\")\n",
    "    task4=task3[0]\n",
    "    task5=task4.replace(\"\"\"\\\"\"\"\",\"\")\n",
    "    \n",
    "\n",
    "    content=task5\n",
    "    \n",
    "    match = re.search(r'[\\w\\.-]+@[\\w\\.-]+',content)\n",
    "    if match != None :\n",
    "        Email=match.group(0)\n",
    "    else :\n",
    "        Email= \"No Email\"\n",
    "    \n",
    "    names_re = re.compile(r'|'.join(re.escape(Loca) for Loca in location))\n",
    "    Located = names_re.search(content)\n",
    "    if Located != None :\n",
    "        Province= Located.group(0)\n",
    "    else :\n",
    "        Province= \"NO Province\"       \n",
    "        \n",
    "    Cell_Phone_No = re.search(r'0[\\d]{2}[-]?[ ]?[\\d]{3}[-]?[ ]?[\\d]{4}',content)\n",
    "    if Cell_Phone_No != None :\n",
    "        CellphoneNo= Cell_Phone_No.group(0)\n",
    "    else :\n",
    "        CellphoneNo= \"NO Cell_Phone_No\"\n",
    "\n",
    "    Phone_No = re.search(r'02[-]?[ ]?[\\d]{3}[-]?[ ]?[\\d]{4}',content)\n",
    "    if Phone_No != None :\n",
    "        PhoneNo= Phone_No.group(0)\n",
    "    else :\n",
    "        PhoneNo= \"NO Phone_No\"\n",
    "\n",
    "    Line_ID=re.search(r'[L,l][I,i][N,n][E,e][ ]?[I,i]?[D,d]?[ ]?[:]?[ ]?[@]?[\\w\\.-]+',content) or re.search(r'[ไ][ล][น][์]?[ไ]?[อ]?[ด]?[ี]?[ ]?[\\w\\.-]+',content) or re.search(r'[ใ][ล][น][์]?[ไ]?[อ]?[ด]?[ี]?[ ]?[\\w\\.-]+',content) or re.search(r'[ล][า][ย][น]?[์]?[ไ]?[อ]?[ด]?[ี]?[ ]?[\\w\\.-]+',content)\n",
    "    if Line_ID != None:\n",
    "        LineID=Line_ID.group(0)\n",
    "    else:\n",
    "        LineID=\"No Line_ID\"\n",
    " \n",
    "    Soii=re.search(r'ซ[., ][\\S\\.-]+',content) or re.search(r'ซอย[., ][\\S\\.-]+',content)\n",
    "    if '0'or'1'or'2'or'3'or'4'or'5'or'6'or'7'or'8'or'9' not in Soii[0]:\n",
    "        Soii=re.search(r'ซ[., ][\\S\\.-]+[ ][\\d\\.-]+',content) or re.search(r'ซอย[., ][\\S\\.-]+[ ][\\d\\.-]+',content)\n",
    "    if Soii != None:\n",
    "        Soi=Soii.group(0)\n",
    "    else:\n",
    "        Soi=\"No Soi\"\n",
    "        \n",
    "    Roads=re.search(r'ถ[., ][\\S\\.-]+',content) or re.search(r'ถนน[., ]?[\\S\\.-]+',content)\n",
    "    if Roads != None:\n",
    "        Road=Roads.group(0)\n",
    "    else:\n",
    "        Road=\"No Road\"\n",
    "           \n",
    "    Distr = re.compile(r'|'.join(re.escape(dis) for dis in Districts))\n",
    "    District = Distr.search(content) or re.search(r'อ[., ][\\S\\.-]+',content) or re.search(r'อำเภอ[., ]?[\\S\\.-]+',content)\n",
    "    if District != None :\n",
    "        district= District.group(0)\n",
    "    else :\n",
    "        district= \"NO District\"  \n",
    "    \n",
    "    SubDistr = re.compile(r'|'.join(re.escape(subdis) for subdis in BkkSubDistricts))\n",
    "    Sub_District = re.search(r'ต[.][ ]?[\\S\\.-]+',content) or re.search(r'ตำบล[.]?[ ]?[\\S\\.-]+',content) or SubDistr.search(content)\n",
    "    if Sub_District != None :\n",
    "        Subdistrict= Sub_District.group(0)\n",
    "    else :\n",
    "        Subdistrict= \"NO Sub District\"  \n",
    "        \n",
    "        \n",
    "\n",
    "    return jsonify({'content':content,'Email':Email,'Line ID':LineID,'Cellphone No':CellphoneNo,'Phone No':PhoneNo,'Soi':Soi,'Sub District':Subdistrict,'District':district,'Road':Road,'Province':Province})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
